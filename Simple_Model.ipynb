{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_directory = 'data/'\n",
    "image_directory = 'data/Pre_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit(np.arange(3).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.transform([[0], [1], [2]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 32\n",
    "batch_size = 256\n",
    "num_channels = 3\n",
    "training_iters = 500\n",
    "display = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_label_files(csv_file):\n",
    "    file = open(data_directory+csv_file, 'r')\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    for i in file:\n",
    "        filename, label = i.split(',')\n",
    "        filepaths.append(image_directory+filename)\n",
    "        labels.append(int(label))\n",
    "    return filepaths, labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path, train_labels = read_label_files('train_set.csv')\n",
    "vali_path, vali_labels = read_label_files('vali_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vali_batch_size = len(vali_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path_tensor = ops.convert_to_tensor(train_path, dtype=dtypes.string)\n",
    "vali_path_tensor = ops.convert_to_tensor(vali_path, dtype=dtypes.string)\n",
    "train_labels_tensor = ops.convert_to_tensor(train_labels, dtype=dtypes.int32)\n",
    "vali_labels_tensor = ops.convert_to_tensor(vali_labels, dtype=dtypes.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input_queue = tf.train.slice_input_producer(\n",
    "                                    [train_path_tensor, train_labels_tensor],\n",
    "                                    shuffle=True)\n",
    "vali_input_queue = tf.train.slice_input_producer(\n",
    "                                    [vali_path_tensor, vali_labels_tensor],\n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_content = tf.read_file(train_input_queue[0])\n",
    "train_image = tf.image.decode_jpeg(file_content, channels=num_channels)\n",
    "train_image /= 255\n",
    "train_label = train_input_queue[1]\n",
    "\n",
    "file_content = tf.read_file(vali_input_queue[0])\n",
    "vali_image = tf.image.decode_jpeg(file_content, channels=num_channels)\n",
    "vali_image /= 255\n",
    "vali_label = vali_input_queue[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_image.set_shape([size, size, num_channels])\n",
    "vali_image.set_shape([size, size, num_channels])\n",
    "\n",
    "train_batch = tf.train.batch(\n",
    "                            [train_image, train_label],\n",
    "                            batch_size= batch_size\n",
    "                            )\n",
    "vali_batch = tf.train.batch(\n",
    "                            [vali_image, vali_label],\n",
    "                            batch_size = vali_batch_size\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "#     for i in range(1):\n",
    "#         test = sess.run(train_batch)\n",
    "#     coord.request_stop()\n",
    "#     coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image, label = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(image.shape)\n",
    "# print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, size, size, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "        'wc1':tf.Variable(tf.random_normal([5, 5, 3, 32])),\n",
    "        'wc2':tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "        'wc3':tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "        'wf1':tf.Variable(tf.random_normal([8 * 8 * 64, 1024])),\n",
    "        'out':tf.Variable(tf.random_normal([1024, 3]))\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bias = {\n",
    "        'bc1' : tf.Variable(tf.random_normal([32])),\n",
    "        'bc2' : tf.Variable(tf.random_normal([64])),\n",
    "        'bc3' : tf.Variable(tf.random_normal([64])),\n",
    "        'bf1' : tf.Variable(tf.random_normal([1024])),\n",
    "        'out':tf.Variable(tf.random_normal([3]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(input_image):\n",
    "        conv1 = tf.nn.conv2d(input_image, weights['wc1'], [1, 1, 1, 1], padding='SAME')\n",
    "        conv1 = tf.add(conv1, bias['bc1'])\n",
    "        max1 = tf.nn.max_pool(conv1, [1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv2 = tf.nn.conv2d(max1, weights['wc2'], [1, 1, 1, 1], padding='SAME')\n",
    "        conv2 = tf.add(conv2, bias['bc2'])\n",
    "        conv3 = tf.nn.conv2d(conv2, weights['wc3'], [1, 1, 1, 1], padding='SAME')\n",
    "        conv3 = tf.add(conv3, bias['bc3'])\n",
    "        max2 = tf.nn.max_pool(conv3, [1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        b, h, w, c = max2.get_shape().as_list()\n",
    "        unrolled = tf.reshape(max2, [-1, h * w * c])\n",
    "        full1 = tf.add(tf.matmul(unrolled, weights['wf1']), bias['bf1'])\n",
    "        out = tf.add(tf.matmul(full1, weights['out']), bias['out'])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Add_4:0' shape=(6, 3) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_net(tf.random_normal([6, 32, 32, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = conv_net(X)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "corrected_pred = tf.equal(tf.arg_max(pred, 1), tf.arg_max(y, 1))\n",
    "acc = tf.reduce_mean(tf.cast(corrected_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "Iter 0, Validation Acc 0.1350, Training loss 4772717.5000\n",
      "Iter 10, Validation Acc 0.3800, Training loss 2992505.5000\n",
      "Iter 20, Validation Acc 0.4050, Training loss 1536617.6250\n",
      "Iter 30, Validation Acc 0.4800, Training loss 686661.2500\n",
      "Iter 40, Validation Acc 0.5150, Training loss 536977.2500\n",
      "Iter 50, Validation Acc 0.5100, Training loss 530826.3750\n",
      "Iter 60, Validation Acc 0.5450, Training loss 420337.0938\n",
      "Iter 70, Validation Acc 0.5650, Training loss 357654.2500\n",
      "Iter 80, Validation Acc 0.5550, Training loss 334200.7812\n",
      "Iter 90, Validation Acc 0.5900, Training loss 312258.2500\n",
      "Iter 100, Validation Acc 0.5950, Training loss 311672.2812\n",
      "Iter 110, Validation Acc 0.5000, Training loss 390904.5625\n",
      "Iter 120, Validation Acc 0.5000, Training loss 331764.0312\n",
      "Iter 130, Validation Acc 0.5600, Training loss 266113.6250\n",
      "Iter 140, Validation Acc 0.6050, Training loss 309179.1250\n",
      "Iter 150, Validation Acc 0.5000, Training loss 306620.3438\n",
      "Iter 160, Validation Acc 0.5950, Training loss 262884.2500\n",
      "Iter 170, Validation Acc 0.5650, Training loss 232373.3594\n",
      "Iter 180, Validation Acc 0.6400, Training loss 252829.4375\n",
      "Iter 190, Validation Acc 0.6100, Training loss 243359.6250\n",
      "Iter 200, Validation Acc 0.5850, Training loss 222847.4844\n",
      "Iter 210, Validation Acc 0.5100, Training loss 282363.3750\n",
      "Iter 220, Validation Acc 0.5550, Training loss 216871.9219\n",
      "Iter 230, Validation Acc 0.5700, Training loss 222149.9531\n",
      "Iter 240, Validation Acc 0.6000, Training loss 206349.5625\n",
      "Iter 250, Validation Acc 0.6150, Training loss 192430.0625\n",
      "Iter 260, Validation Acc 0.6100, Training loss 215191.1406\n",
      "Iter 270, Validation Acc 0.5200, Training loss 203423.9062\n",
      "Iter 280, Validation Acc 0.5600, Training loss 216068.3750\n",
      "Iter 290, Validation Acc 0.5750, Training loss 190502.7188\n",
      "Iter 300, Validation Acc 0.5800, Training loss 192660.1406\n",
      "Iter 310, Validation Acc 0.5700, Training loss 189514.0469\n",
      "Iter 320, Validation Acc 0.5750, Training loss 190638.6875\n",
      "Iter 330, Validation Acc 0.5900, Training loss 181523.4062\n",
      "Iter 340, Validation Acc 0.5400, Training loss 200121.8438\n",
      "Iter 350, Validation Acc 0.5450, Training loss 179052.5781\n",
      "Iter 360, Validation Acc 0.5750, Training loss 182513.0781\n",
      "Iter 370, Validation Acc 0.5650, Training loss 171747.3125\n",
      "Iter 380, Validation Acc 0.6000, Training loss 181873.2031\n",
      "Iter 390, Validation Acc 0.5750, Training loss 162671.3438\n",
      "Iter 400, Validation Acc 0.5550, Training loss 180006.4375\n",
      "Iter 410, Validation Acc 0.4450, Training loss 378457.8438\n",
      "Iter 420, Validation Acc 0.4700, Training loss 260283.0469\n",
      "Iter 430, Validation Acc 0.5650, Training loss 174047.4375\n",
      "Iter 440, Validation Acc 0.6100, Training loss 152846.4844\n",
      "Iter 450, Validation Acc 0.5050, Training loss 277717.2500\n",
      "Iter 460, Validation Acc 0.5400, Training loss 163384.1250\n",
      "Iter 470, Validation Acc 0.5950, Training loss 166494.1875\n",
      "Iter 480, Validation Acc 0.5350, Training loss 187536.7656\n",
      "Iter 490, Validation Acc 0.5000, Training loss 255355.4375\n",
      "Saved and Optimized\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    val_img, val_lab = sess.run(vali_batch)\n",
    "    val_lab = ohe.transform(val_lab.reshape(-1, 1)).toarray()\n",
    "    print(val_lab.shape)\n",
    "    \n",
    "    for i in range(training_iters):\n",
    "        image, label = sess.run(train_batch)\n",
    "        label = ohe.transform(label.reshape(-1, 1)).toarray()\n",
    "        temp, _ = sess.run([loss, train_op], feed_dict={X:image, y:label})\n",
    "        if i % display == 0:\n",
    "            val_loss, accuracy = sess.run([loss, acc], feed_dict={X:val_img, y:val_lab})\n",
    "            save_path = saver.save(sess, \"models/model.ckpt\")\n",
    "            print('Iter %s, Validation Acc %.4f, Training loss %.4f'%(i, accuracy, val_loss))\n",
    "    print('Saved and Optimized')\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
