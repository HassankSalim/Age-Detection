{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.python.framework import graph_util\n",
    "from os.path import isfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_directory = 'data/'\n",
    "image_directory = 'data/Pre_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit(np.arange(3).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.transform([[0], [1], [2]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 32\n",
    "batch_size = 256\n",
    "num_channels = 3\n",
    "training_iters = 1001\n",
    "display = 10\n",
    "drop = 0.75\n",
    "save_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_label_files(csv_file):\n",
    "    file = open(data_directory+csv_file, 'r')\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    for i in file:\n",
    "        filename, label = i.split(',')\n",
    "        filepaths.append(image_directory+filename)\n",
    "        labels.append(int(label))\n",
    "    return filepaths, labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path, train_labels = read_label_files('train_set.csv')\n",
    "vali_path, vali_labels = read_label_files('vali_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vali_batch_size = len(vali_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path_tensor = ops.convert_to_tensor(train_path, dtype=dtypes.string)\n",
    "vali_path_tensor = ops.convert_to_tensor(vali_path, dtype=dtypes.string)\n",
    "train_labels_tensor = ops.convert_to_tensor(train_labels, dtype=dtypes.int32)\n",
    "vali_labels_tensor = ops.convert_to_tensor(vali_labels, dtype=dtypes.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input_queue = tf.train.slice_input_producer(\n",
    "                                    [train_path_tensor, train_labels_tensor],\n",
    "                                    shuffle=True)\n",
    "vali_input_queue = tf.train.slice_input_producer(\n",
    "                                    [vali_path_tensor, vali_labels_tensor],\n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_content = tf.read_file(train_input_queue[0])\n",
    "train_image = tf.image.decode_jpeg(file_content, channels=num_channels)\n",
    "train_image /= 255\n",
    "train_label = train_input_queue[1]\n",
    "\n",
    "file_content = tf.read_file(vali_input_queue[0])\n",
    "vali_image = tf.image.decode_jpeg(file_content, channels=num_channels)\n",
    "vali_image /= 255\n",
    "vali_label = vali_input_queue[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_image.set_shape([size, size, num_channels])\n",
    "vali_image.set_shape([size, size, num_channels])\n",
    "\n",
    "train_batch = tf.train.batch(\n",
    "                            [train_image, train_label],\n",
    "                            batch_size= batch_size\n",
    "                            )\n",
    "vali_batch = tf.train.batch(\n",
    "                            [vali_image, vali_label],\n",
    "                            batch_size = vali_batch_size\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freeze_graph(model_folder):\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_folder)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "    model_name = 'model_v3'\n",
    "    absolute_model_folder = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_folder + \"/\" + model_name + \".pb\"\n",
    "\n",
    "    output_node_names = \"final_output\"\n",
    "\n",
    "    clear_devices = True\n",
    "\n",
    "    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    input_graph_def = graph.as_graph_def()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "\n",
    "        output_graph_def = graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            input_graph_def,\n",
    "            output_node_names.split(\",\")\n",
    "        )\n",
    "\n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "#     for i in range(1):\n",
    "#         test = sess.run(train_batch)\n",
    "#     coord.request_stop()\n",
    "#     coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image, label = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(image.shape)\n",
    "# print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, size, size, 3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32, shape=[None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "        'wc1':tf.Variable(tf.random_normal([5, 5, 3, 32])),\n",
    "        'wc2':tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "        'wc3':tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "        'wf1':tf.Variable(tf.random_normal([8 * 8 * 64, 1024])),\n",
    "        'wf2':tf.Variable(tf.random_normal([1024, 100])),\n",
    "        'out':tf.Variable(tf.random_normal([100, 3]))\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bias = {\n",
    "        'bc1' : tf.Variable(tf.random_normal([32])),\n",
    "        'bc2' : tf.Variable(tf.random_normal([64])),\n",
    "        'bc3' : tf.Variable(tf.random_normal([64])),\n",
    "        'bf1' : tf.Variable(tf.random_normal([1024])),\n",
    "        'bf2' : tf.Variable(tf.random_normal([100])),\n",
    "        'out':tf.Variable(tf.random_normal([3]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(input_image):\n",
    "        conv1 = tf.nn.conv2d(input_image, weights['wc1'], [1, 1, 1, 1], padding='SAME')\n",
    "        conv1 = tf.add(conv1, bias['bc1'])\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "        max1 = tf.nn.max_pool(conv1, [1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv2 = tf.nn.conv2d(max1, weights['wc2'], [1, 1, 1, 1], padding='SAME')\n",
    "        conv2 = tf.add(conv2, bias['bc2'])\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "        max2 = tf.nn.max_pool(conv2, [1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv3 = tf.nn.conv2d(max2, weights['wc3'], [1, 1, 1, 1], padding='SAME')\n",
    "        conv3 = tf.add(conv3, bias['bc3'])\n",
    "        conv3 = tf.nn.relu(conv3)\n",
    "        b, h, w, c = conv3.get_shape().as_list()\n",
    "        unrolled = tf.reshape(max2, [-1, h * w * c])\n",
    "        full1 = tf.add(tf.matmul(unrolled, weights['wf1']), bias['bf1'])\n",
    "        full1 = tf.nn.relu(full1)\n",
    "        full2 = tf.add(tf.matmul(full1, weights['wf2']), bias['bf2'])\n",
    "        full2 = tf.nn.relu(full2)\n",
    "        out = tf.add(tf.matmul(full2, weights['out']), bias['out'])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Add_5:0' shape=(6, 3) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_net(tf.random_normal([6, 32, 32, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = conv_net(X)\n",
    "output = tf.argmax(pred, axis = 1, name = 'final_output')\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "corrected_pred = tf.equal(tf.arg_max(pred, 1), tf.arg_max(y, 1))\n",
    "acc = tf.reduce_mean(tf.cast(corrected_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "Iter 0, Validation Acc 0.6000, Training loss 14317.7900\n",
      "Iter 10, Validation Acc 0.6150, Training loss 14263.8877\n",
      "Iter 20, Validation Acc 0.6150, Training loss 15565.0234\n",
      "Iter 30, Validation Acc 0.6500, Training loss 14785.6660\n",
      "Iter 40, Validation Acc 0.6550, Training loss 14949.3740\n",
      "Iter 50, Validation Acc 0.6450, Training loss 14490.6084\n",
      "Iter 60, Validation Acc 0.6100, Training loss 15348.9424\n",
      "Iter 70, Validation Acc 0.6400, Training loss 14087.1797\n",
      "Iter 80, Validation Acc 0.6100, Training loss 14375.2383\n",
      "Iter 90, Validation Acc 0.5950, Training loss 14998.0791\n",
      "Iter 100, Validation Acc 0.5850, Training loss 14579.5879\n",
      "Iter 110, Validation Acc 0.6100, Training loss 14946.0752\n",
      "Iter 120, Validation Acc 0.5900, Training loss 15233.0752\n",
      "Iter 130, Validation Acc 0.5650, Training loss 14729.9600\n",
      "Iter 140, Validation Acc 0.5850, Training loss 14995.1602\n",
      "Iter 150, Validation Acc 0.6100, Training loss 13370.7109\n",
      "Iter 160, Validation Acc 0.6150, Training loss 13503.6475\n",
      "Iter 170, Validation Acc 0.6550, Training loss 13943.6611\n",
      "Iter 180, Validation Acc 0.6450, Training loss 14619.0859\n",
      "Iter 190, Validation Acc 0.6400, Training loss 14347.6826\n",
      "Iter 200, Validation Acc 0.6250, Training loss 13495.7383\n",
      "Iter 210, Validation Acc 0.6400, Training loss 15950.3154\n",
      "Iter 220, Validation Acc 0.6400, Training loss 13425.1172\n",
      "Iter 230, Validation Acc 0.6200, Training loss 15273.4746\n",
      "Iter 240, Validation Acc 0.6100, Training loss 12357.2529\n",
      "Iter 250, Validation Acc 0.5900, Training loss 13975.6172\n",
      "Iter 260, Validation Acc 0.6550, Training loss 14342.6055\n",
      "Iter 270, Validation Acc 0.6600, Training loss 16033.9326\n",
      "Iter 280, Validation Acc 0.6850, Training loss 14657.8223\n",
      "Iter 290, Validation Acc 0.6700, Training loss 14314.5215\n",
      "Iter 300, Validation Acc 0.5900, Training loss 13974.7900\n",
      "Iter 310, Validation Acc 0.5850, Training loss 14527.6621\n",
      "Iter 320, Validation Acc 0.5700, Training loss 15038.6338\n",
      "Iter 330, Validation Acc 0.6250, Training loss 12975.9736\n",
      "Iter 340, Validation Acc 0.6400, Training loss 14648.5049\n",
      "Iter 350, Validation Acc 0.6400, Training loss 12299.7549\n",
      "Iter 360, Validation Acc 0.6400, Training loss 12893.6748\n",
      "Iter 370, Validation Acc 0.6200, Training loss 12831.8828\n",
      "Iter 380, Validation Acc 0.6500, Training loss 13741.1436\n",
      "Iter 390, Validation Acc 0.6350, Training loss 13460.8555\n",
      "Iter 400, Validation Acc 0.6450, Training loss 13067.6377\n",
      "Iter 410, Validation Acc 0.6000, Training loss 13296.6816\n",
      "Iter 420, Validation Acc 0.6400, Training loss 13089.7129\n",
      "Iter 430, Validation Acc 0.5950, Training loss 12262.0625\n",
      "Iter 440, Validation Acc 0.6400, Training loss 13096.0674\n",
      "Iter 450, Validation Acc 0.6350, Training loss 13178.0498\n",
      "Iter 460, Validation Acc 0.6250, Training loss 13926.5576\n",
      "Iter 470, Validation Acc 0.6200, Training loss 13896.8926\n",
      "Iter 480, Validation Acc 0.6300, Training loss 13689.4375\n",
      "Iter 490, Validation Acc 0.6400, Training loss 14179.3154\n",
      "Iter 500, Validation Acc 0.6400, Training loss 13733.5820\n",
      "Iter 510, Validation Acc 0.6350, Training loss 12813.0361\n",
      "Iter 520, Validation Acc 0.6450, Training loss 12662.8799\n",
      "Iter 530, Validation Acc 0.6350, Training loss 12784.2646\n",
      "Iter 540, Validation Acc 0.6500, Training loss 12890.8760\n",
      "Iter 550, Validation Acc 0.6200, Training loss 12714.5762\n",
      "Iter 560, Validation Acc 0.6050, Training loss 12534.7354\n",
      "Iter 570, Validation Acc 0.6350, Training loss 13394.3574\n",
      "Iter 580, Validation Acc 0.6500, Training loss 13089.8428\n",
      "Iter 590, Validation Acc 0.6300, Training loss 13682.4102\n",
      "Iter 600, Validation Acc 0.6100, Training loss 12287.6172\n",
      "Iter 610, Validation Acc 0.6200, Training loss 12715.8076\n",
      "Iter 620, Validation Acc 0.6050, Training loss 12629.9248\n",
      "Iter 630, Validation Acc 0.6250, Training loss 12810.9854\n",
      "Iter 640, Validation Acc 0.6450, Training loss 13716.8379\n",
      "Iter 650, Validation Acc 0.6300, Training loss 13662.7334\n",
      "Iter 660, Validation Acc 0.6600, Training loss 13408.1201\n",
      "Iter 670, Validation Acc 0.6500, Training loss 12785.3633\n",
      "Iter 680, Validation Acc 0.6350, Training loss 12588.8584\n",
      "Iter 690, Validation Acc 0.6350, Training loss 12678.5000\n",
      "Iter 700, Validation Acc 0.6300, Training loss 12726.3975\n",
      "Iter 710, Validation Acc 0.6050, Training loss 12284.1279\n",
      "Iter 720, Validation Acc 0.6200, Training loss 12402.6504\n",
      "Iter 730, Validation Acc 0.6200, Training loss 12621.1826\n",
      "Iter 740, Validation Acc 0.6250, Training loss 12700.8516\n",
      "Iter 750, Validation Acc 0.6300, Training loss 12599.1484\n",
      "Iter 760, Validation Acc 0.6400, Training loss 11983.0127\n",
      "Iter 770, Validation Acc 0.6400, Training loss 12648.0439\n",
      "Iter 780, Validation Acc 0.5900, Training loss 14380.0479\n",
      "Iter 790, Validation Acc 0.6200, Training loss 13271.3555\n",
      "Iter 800, Validation Acc 0.6450, Training loss 12679.0342\n",
      "Iter 810, Validation Acc 0.6350, Training loss 12383.7178\n",
      "Iter 820, Validation Acc 0.6400, Training loss 12279.1172\n",
      "Iter 830, Validation Acc 0.6300, Training loss 12698.8877\n",
      "Iter 840, Validation Acc 0.6300, Training loss 12589.6064\n",
      "Iter 850, Validation Acc 0.6500, Training loss 12623.9102\n",
      "Iter 860, Validation Acc 0.6600, Training loss 12462.6562\n",
      "Iter 870, Validation Acc 0.6000, Training loss 14225.2227\n",
      "Iter 880, Validation Acc 0.6350, Training loss 12375.5498\n",
      "Iter 890, Validation Acc 0.6500, Training loss 12733.4717\n",
      "Iter 900, Validation Acc 0.6000, Training loss 14515.2070\n",
      "Iter 910, Validation Acc 0.6250, Training loss 14312.2422\n",
      "Iter 920, Validation Acc 0.5900, Training loss 12208.6562\n",
      "Iter 930, Validation Acc 0.6350, Training loss 11997.4199\n",
      "Iter 940, Validation Acc 0.5900, Training loss 12977.3555\n",
      "Iter 950, Validation Acc 0.6200, Training loss 13163.7148\n",
      "Iter 960, Validation Acc 0.6400, Training loss 12162.7549\n",
      "Iter 970, Validation Acc 0.6550, Training loss 12033.6172\n",
      "Iter 980, Validation Acc 0.6600, Training loss 12800.3496\n",
      "Iter 990, Validation Acc 0.6250, Training loss 12678.6523\n",
      "Iter 1000, Validation Acc 0.6500, Training loss 12545.1875\n",
      "Saved and Optimized\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    if isfile('models/model_1.ckpt.index'):\n",
    "        saver.restore(sess, 'models/model_1.ckpt')\n",
    "    val_img, val_lab = sess.run(vali_batch)\n",
    "    val_lab = ohe.transform(val_lab.reshape(-1, 1)).toarray()\n",
    "    print(val_lab.shape)\n",
    "    for i in range(training_iters):\n",
    "        image, label = sess.run(train_batch)\n",
    "        label = ohe.transform(label.reshape(-1, 1)).toarray()\n",
    "        temp, _ = sess.run([loss, train_op], feed_dict={X:image, y:label, keep_prob:drop})\n",
    "        if i % display == 0:\n",
    "            val_loss, accuracy = sess.run([loss, acc], feed_dict={X:val_img, y:val_lab, keep_prob:1.0})\n",
    "            print('Iter %s, Validation Acc %.4f, Training loss %.4f'%(i, accuracy, val_loss))  \n",
    "            save_path = saver.save(sess, \"models/model_1.ckpt\")\n",
    "    print('Saved and Optimized')\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 10 variables.\n",
      "Converted 10 variables to const ops.\n",
      "41 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
